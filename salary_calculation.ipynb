{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613d7b8d-cbc6-4ce1-9911-7eac77f314bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark transformation basics\").getOrCreate()\n",
    "df = spark.read.csv(\"/Volumes/workspace/default/pyspark_sample/Employee_salary.csv\", header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e76834-9a3b-4c3a-b7ca-a5ce7f334796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumnRenamed(\"Dept\",\"Departments\")\n",
    "df=df.withColumnRenamed(\"EmpID\",\"EmployeeID\")\n",
    "df=df.withColumn(\"Bonus\",df.Salary*0.2)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b34cd90-6c2b-4525-9f50-d9543e9b6dfd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.withColumn(\"Total_salary\",df.Salary+df.Bonus)\n",
    "df=df.drop(\"Total salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b4b8db-0f3c-4b66-b38e-31e354ccb5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1.add column and cast the value\n",
    "df=df.withColumn(\"Total_salary\",df.Total_salary.cast(\"int\"))\n",
    "df=df.withColumn(\"Bonus\",df.Bonus.cast(\"Double\"))\n",
    "df=df.withColumn(\"Salary\",df.Salary.cast(\"Double\"))\n",
    "df=df.withColumn(\"Experience\",df.Experience.cast(\"int\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58e08536-1894-4883-a8b1-1fd170721e11",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 5"
    }
   },
   "outputs": [],
   "source": [
    "#2.Filter and add column\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df.filter(df.Salary > 45000)\n",
    "df.show()\n",
    "df = df.withColumn(\n",
    "    \"Level\",\n",
    "    when(df.Experience >= 5, \"Lead\")\n",
    "    .otherwise(when(df.Experience > 3, \"Senior\")\n",
    "    .otherwise(\"Junior\"))\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fb34ec51-ba1a-411d-ba77-8973227ea275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#aggregation functions and GroupBy\n",
    "df.groupBy(\"Departments\").count().show()\n",
    "df.groupBy(\"Departments\").agg({\"Salary\": \"sum\"}).show()\n",
    "df.groupby(\"City\").agg({\"Salary\": \"avg\"}).show()\n",
    "df.groupby(\"Departments\").count().show()\n",
    "df.groupby(\"Departments\").agg({\"Salary\": \"avg\"}).show()\n",
    "df.groupby(\"Departments\").agg({\"Salary\": \"max\"}).show()\n",
    "df.groupby(\"Departments\").agg({\"Salary\": \"min\"}).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "87271ca8-fe55-4ef2-a87d-0dbc81b1559f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 7"
    }
   },
   "outputs": [],
   "source": [
    "# Window function\n",
    "from pyspark.sql.window import Window\n",
    "#from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.sql.functions import rank\n",
    "windowspec=Window.partitionBy(\"Departments\").orderBy(col(\"Salary\").desc())\n",
    "df=df.withColumn(\"rank\",rank().over(windowspec))\n",
    "df.show()\n",
    "df.filter(df.rank==1).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "fd54c91a-1f3c-4109-b273-5a22f4890939",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 8"
    }
   },
   "outputs": [],
   "source": [
    "#join\n",
    "df1 = spark.read.csv(\"/Volumes/workspace/default/pyspark_sample/Employee_salary.csv\", header=True, inferSchema=True)\n",
    "df1.show()\n",
    "\n",
    "departments = [\n",
    "    {\"Dept\": \"HR\", \"location\": \"Block A\"},\n",
    "    {\"Dept\": \"IT\", \"location\": \"Block B\"},\n",
    "    {\"Dept\": \"Sales\", \"location\": \"Block C\"},\n",
    "    {\"Dept\": \"Marketing\", \"location\": \"Block D\"},\n",
    "    {\"Dept\": \"Finance\", \"location\": \"Block E\"},\n",
    "    {\"Dept\": \"Engineering\", \"location\": \"Block F\"},\n",
    "]\n",
    "\n",
    "departments_df = spark.createDataFrame(departments)\n",
    "df1.join(departments_df, df1.Dept == departments_df.Dept, \"inner\").show()\n",
    "df1.join(broadcast(departments_df), \"Dept\", \"inner\").show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "salary_calculation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
